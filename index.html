<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="NL2SQL-Bugs: A Benchmark for Detecting Semantic Errors in NL2SQL Translation.">
  <meta name="keywords" content="NL2SQL, LLMs, Error Detection, Semantic Errors, Database">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>NL2SQL-Bugs: A Benchmark for Detecting Semantic Errors in NL2SQL Translation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/diallab.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/HKUSTDial?q=nvbench&type=all&language=&sort=">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          DIAL Lab More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://github.com/HKUSTDial/NL2SQL_Handbook">
            NL2SQL Handbook
          </a>
          <a class="navbar-item" href="https://nl2sql360.github.io/">
            NL2SQL360
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">NL2SQL-Bugs: A Benchmark for Detecting Semantic Errors in NL2SQL Translation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Xinyu Liu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Shuyu Shen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Boyan Li</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Nan Tang</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Yuyu Luo</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>The Hong Kong University of Science and Technology (Guangzhou)</span>
          </div>
          

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.11984"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/HKUSTDial/NL2SQL-Bugs-Benchmark"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href="https://github.com/HKUSTDial/NL2SQL-Bugs-Benchmark"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Natural Language to SQL (i.e., NL2SQL) translation is crucial for democratizing database access, but even SOTA models frequently generate semantically incorrect SQL queries, hindering the widespread adoption of these techniques by database vendors. While existing NL2SQL benchmarks primarily focus on correct query translation, we argue that a benchmark dedicated to identifying common errors in NL2SQL translations is equally important, as accurately detecting these errors is a prerequisite for any subsequent correction â€“ whether performed by humans or models.
          </p>
          <p>
            To address this gap, we propose NL2SQL-BUGs, the first benchmark dedicated to detecting and categorizing semantic errors in NL2SQL translation. NL2SQL-BUGs adopts a two-level taxonomy to systematically classify semantic errors, covering 9 main categories and 31 subcategories. The benchmark consists of 2,018 expert-annotated instances, each containing a natural language query, database schema, and SQL query, with detailed error annotations for semantically incorrect queries.
          </p>
          <p>
            Through comprehensive experiments, we demonstrate that current large language models exhibit significant limitations in semantic error detection, achieving an average detection accuracy of only 75.16%. Despite this, the models were able to successfully detect 106 errors (accounting for 6.91%) in the widely-used NL2SQL dataset, BIRD, which were previously annotation errors in the benchmark. This highlights the importance of semantic error detection in NL2SQL systems.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Taxonomy Overview. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Error Taxonomy</h2>
        <div class="content has-text-justified">
          <p>
            The classification of semantic errors in NL2SQL is based on the structure of SQL queries, common translation mistakes, and their impact on query semantics. This approach allows for systematic error identification at various stages of query generation, helping to pinpoint where and why translation mistakes occur.
          </p>
          <p>
            Our taxonomy classifies semantic errors into 9 main categories and 31 subcategories:
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-full">
            <figure class="image">
              <img src="./static/images/error_taxonomy.png" alt="NL2SQL Error Taxonomy">
              <figcaption class="has-text-centered">Figure 1: A Taxonomy of NL2SQL Translation Semantic Errors</figcaption>
            </figure>
          </div>
        </div>
        <br/>
        <!--/ Taxonomy Overview. -->
      </div>
    </div>
  </div>
  <div class="container is-max-desktop">
    <!-- Taxonomy Overview. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">NL2SQL-BUGs Statistics</h2>
        <div class="content has-text-justified">
          <p>
            We collect 2,018 expert-annotated instances, each containing a natural language query, database schema, and SQL query. Among these instances, 1,019 are correct examples while 999 are incorrect examples with semantic errors. Each incorrect example is carefully annotated with detailed error types and explanations, providing a comprehensive resource for studying semantic errors in NL2SQL translation.
          </p>
          <p>
            The statistics of NL2SQL-BUGs are shown in the following figure:
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-full">
            <figure class="image">
              <img src="./static/images/static.svg" alt="NL2SQL Error Taxonomy">
              <figcaption class="has-text-centered">Figure 2: Statistics of NL2SQL-BUGs</figcaption>
            </figure>
          </div>
        </div>
        <br/>
        <!--/ Taxonomy Overview. -->
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Experiment. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Experiments</h2>
        <div class="content has-text-justified">
          <p>
            Through comprehensive experiments on NL2SQL-BUGs, we evaluate the semantic error detection capabilities of state-of-the-art large language models (LLMs). Our results reveal significant limitations in their performance, with an average detection accuracy of only 75.16%. Among the tested models, GPT-4o and Claude-3.5-Sonnet demonstrate the most balanced performance, showing consistent capability across various types of semantic errors. Interestingly, while Gemini-2.0-Flash achieves high positive recall, it struggles with controlling false positives, indicating a potential trade-off between sensitivity and precision in error detection.
          </p>
          <p>
            Our fine-grained analysis across error types reveals notable patterns in model performance. The models show particular strength in identifying certain error categories, especially condition-related errors and value errors. However, they consistently struggle with more complex error types, particularly subquery-related errors and those requiring deep database knowledge, such as join type mismatches and function-related errors. This suggests that current LLMs, despite their impressive capabilities, still lack sophisticated understanding of database operations and complex query structures.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-three-quarters">
            <figure class="image">
              <img src="./static/images/error_detection.png" alt="LLM Performance on NL2SQL-BUGs">
              <figcaption class="has-text-centered">Figure 3: Semantic Error Detection Performance</figcaption>
            </figure>
          </div>
        </div>
        <br/>
        <div class="columns is-centered">
          <div class="column is-three-quarters">
            <figure class="image">
              <img src="./static/images/type_detection.png" alt="Error Type Detection Performance">
              <figcaption class="has-text-centered">Figure 4: Error Type Detection Performance</figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
    <!--/ Experiment. -->
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Errors in Existing Benchmarks -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Errors Found in Existing Benchmarks</h2>
        <div class="content has-text-justified">
          <p>
            After applying our automated semantic error detection framework and conducting manual validation of the LLM's predictions, we discovered previously unidentified semantic errors in widely-used NL2SQL benchmarks. 
          </p>
        </div>
        
        <div class="columns">
          <!-- Left Column: Text Description -->
          <div class="column is-half">
            <div class="content has-text-justified">
              <p>
                There are 16 SQL queries in Spider (1.55% of the development set) and 106 SQL queries in BIRD (6.91% of the development set) contained semantic errors that had not been previously identified. These findings demonstrate both the utility of our semantic error detection model and the importance of systematic error detection in NL2SQL datasets.
              </p>
              
              <p>
                These findings highlight the importance of rigorous validation in NL2SQL benchmark creation and the need for systematic error detection approaches. 
              </p>
            </div>
          </div>
          
          <!-- Right Column: Error Statistics -->
          <div class="column is-half">
            <figure class="image">
              <img src="./static/images/error_idx.png" alt="Error Statistics in Benchmarks">
              <figcaption class="has-text-centered">Figure 5: Semantic Errors in BIRD and Spider Benchmarks</figcaption>
            </figure>
            <br/>
            
          </div>
        </div>
        <figure class="image">
          <img src="./static/images/case_study.png" alt="Error Examples">
          <figcaption class="has-text-centered">Figure 6: Examples of Semantic Errors in BIRD and Spider Benchmarks</figcaption>
        </figure>
      </div>
    </div>
    <!--/ Errors in Existing Benchmarks -->
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{liu2025nl2sqlbugsbenchmarkdetectingsemantic,
      title={NL2SQL-BUGs: A Benchmark for Detecting Semantic Errors in NL2SQL Translation}, 
      author={Xinyu Liu and Shuyu Shen and Boyan Li and Nan Tang and Yuyu Luo},
      year={2025},
      eprint={2503.11984},
      archivePrefix={arXiv},
      primaryClass={cs.DB},
      url={https://arxiv.org/abs/2503.11984}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="#"> <!-- TODO: Replace with actual PDF link when available -->
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/nl2sql-bugs/nl2sql-bugs.github.io" class="external-link">
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nl2sql-bugs/nl2sql-bugs.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
